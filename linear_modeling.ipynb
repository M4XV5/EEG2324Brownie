{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Further Reaching Analysis: Linear Modeling\n",
    "\n",
    "For our further reaching analysis, we want to check different predictors and understand how each of them influences the outcome variable Voltage. We discussed about whether to use RewP or voltage (mean/max) as the outcome variable and decided to use voltage. Our rationale is that when we were to use RewP, most of a selected predictors can not be included anymore as RewP is constructed by the a "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4df028a6f9aaeb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# HYPTERPARAMETERS\n",
    "VOLTAGE_TO_ANALYZE = \"mean_voltage\"\n",
    "SELECTED_CHANNEL=\"FCz\"\n",
    "FILT_DS_PATH=\"Dataset/ds004147-filtered\"\n",
    "RAW_DS_PATH=\"Dataset/ds004147\"\n",
    "OUTPUT_DIR=None  # None means don't save as csv\n",
    "SUBJECTS_TO_INCLUDE=None  # None means all\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T17:33:59.107227Z",
     "start_time": "2024-03-29T17:33:59.103Z"
    }
   },
   "id": "8c0f23c593e6d6a5",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import all needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import chi2_contingency\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T17:33:59.164370Z",
     "start_time": "2024-03-29T17:33:59.159136Z"
    }
   },
   "id": "3b6376fbd79f2edc",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "#####################################################  This section is for creating the dummy data... Skip this when we have the real data  ############################################################################################ "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f679d49c54d88277"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dummy Data Generation Function\n",
    "def generate_dummy_values_csv(csv_name, num_values=100, median=0, spread=1, win_ratio=0.5):\n",
    "    rewp_values = np.random.randn(num_values) * spread + median\n",
    "    # Generate win/loss labels directly \n",
    "    prev_outcome = np.random.choice(['loss', 'win'], size=num_values, p=[1-win_ratio, win_ratio])\n",
    "    performance = np.random.uniform(0.5, 1, size=num_values)\n",
    "    outcome = np.random.choice(['loss', 'win'], size=num_values, p=[1-win_ratio, win_ratio])\n",
    "    df = pd.DataFrame({\"outcome\": outcome, 'prev_outcome': prev_outcome, \"performance\": performance, 'RewP': rewp_values})\n",
    "    df.to_csv(csv_name, index=False)\n",
    "  \n",
    "# Generation of Dummy Data as csv\n",
    "generate_dummy_values_csv(\"data/lowlow_rewp_dummy_values.csv\", num_values=100, median=7, spread=3, win_ratio=0.5)\n",
    "generate_dummy_values_csv(\"data/midlow_rewp_dummy_values.csv\", num_values=50, median=7.5, spread=3, win_ratio=0.5)\n",
    "generate_dummy_values_csv(\"data/midhigh_rewp_dummy_values.csv\", num_values=50, median=8, spread=3, win_ratio=0.7)\n",
    "generate_dummy_values_csv(\"data/highhigh_rewp_dummy_values.csv\", num_values=100, median=5.5, spread=3, win_ratio=0.7)\n",
    "\n",
    "# Reading the generated Dummy Data\n",
    "lowlow_df = pd.read_csv('data/lowlow_rewp_dummy_values.csv')\n",
    "midlow_df = pd.read_csv('data/midlow_rewp_dummy_values.csv')\n",
    "midhigh_df = pd.read_csv('data/midhigh_rewp_dummy_values.csv')\n",
    "highhigh_df = pd.read_csv('data/highhigh_rewp_dummy_values.csv')\n",
    "\n",
    "# Creating two new columns Task and Cue and assign the corresponding value to the Dummy Data\n",
    "lowlow_df[\"task\"] = [\"low\" for _ in range(len(lowlow_df))]\n",
    "lowlow_df[\"cue\"] = [\"low\" for _ in range(len(lowlow_df))]\n",
    "midlow_df[\"task\"] = [\"mid\" for _ in range(len(midlow_df))]\n",
    "midlow_df[\"cue\"] = [\"low\" for _ in range(len(midlow_df))]\n",
    "midhigh_df[\"task\"] = [\"mid\" for _ in range(len(midlow_df))]\n",
    "midhigh_df[\"cue\"] = [\"high\" for _ in range(len(midlow_df))]\n",
    "highhigh_df[\"task\"] = [\"high\" for _ in range(len(highhigh_df))]\n",
    "highhigh_df[\"cue\"] = [\"high\" for _ in range(len(highhigh_df))]\n",
    "\n",
    "# Combining Dummy Data\n",
    "df_list = [lowlow_df, midlow_df, midhigh_df, highhigh_df]\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "data.to_csv(\"data/dummy_data.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T17:33:59.252890Z",
     "start_time": "2024-03-29T17:33:59.189011Z"
    }
   },
   "id": "e532aa923218f156",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "####################################################################################################################################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4afbab002a4da6db"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "CONDITION_BEH_MAPPING = {\n",
    "    6: (1, 50, 1),\n",
    "    16: (2, 50, 1),\n",
    "    26: (2, 80, 1),\n",
    "    36: (3, 80, 1),\n",
    "    7: (1, 50, 0),\n",
    "    17: (2, 50, 0),\n",
    "    27: (2, 80, 0),\n",
    "    37: (3, 80, 0),\n",
    "}\n",
    "\n",
    "CONDITION_MAPPING = {\n",
    "    'Win LL': 6,\n",
    "    'Loss LL': 7,\n",
    "    'Win ML': 16,\n",
    "    'Loss ML': 17,\n",
    "    'Win MH': 26,\n",
    "    'Loss MH': 27,\n",
    "    'Win HH': 36,\n",
    "    'Loss HH': 37,\n",
    "}\n",
    "\n",
    "# import all needed helper functions\n",
    "def create_df_from_beh_tsv(tsv_name):\n",
    "    df = pd.read_csv(tsv_name, sep='\\t')\n",
    "    df = df[['block', 'trial', 'task', 'prob', 'outcome', 'optimal', 'rt']]\n",
    "    df['id'] = df['block'] * df['trial']\n",
    "    # Calculate Z-score of RT within each task\n",
    "    task_mean_rt = df.groupby('task')['rt'].transform('mean')\n",
    "    task_std_rt = df.groupby('task')['rt'].transform('std')\n",
    "    df['rt_z_score'] = (df['rt'] - task_mean_rt) / task_std_rt\n",
    "    # Calculate percentile of RT within each task\n",
    "    task_percentiles = df.groupby('task')['rt'].transform(lambda x: (x.rank() - 1) / len(x) * 100)\n",
    "    df['rt_percentile'] = task_percentiles\n",
    "    # Calculate residuals\n",
    "    residuals = df.groupby('task')['rt'].transform(lambda x: x - x.mean())\n",
    "    df['residuals'] = residuals\n",
    "    return df[['id', 'task', 'prob', 'outcome', 'optimal', 'rt_z_score', 'rt_percentile', 'residuals']]\n",
    "\n",
    "def create_outcome_figure(df1, df2, title, ax):\n",
    "    ax.boxplot([df1[VOLTAGE_TO_ANALYZE], df2[VOLTAGE_TO_ANALYZE]])\n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels(['Loss (prev)', 'Win (prev)'])\n",
    "    ax.set_ylabel(VOLTAGE_TO_ANALYZE)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "def analyze_categorical_feature(feature, data, second_feature=None):\n",
    "    if second_feature:\n",
    "        # Pair up the first and second features into one\n",
    "        data[f'{feature}-{second_feature}'] = data[feature] + '-' + data[second_feature]\n",
    "        feature = f'{feature}-{second_feature}'\n",
    "\n",
    "    # Grouping data by the feature and extracting 'RewP' values for each group\n",
    "    feature_groups = data.groupby(feature)[VOLTAGE_TO_ANALYZE].apply(list)\n",
    "    \n",
    "    # Plotting boxplot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.boxplot(feature_groups.values, labels=feature_groups.index)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(VOLTAGE_TO_ANALYZE)\n",
    "    plt.title(f'Boxplot of {VOLTAGE_TO_ANALYZE} by {feature}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    \n",
    "    # Fit ANOVA model\n",
    "    model = ols(f'{VOLTAGE_TO_ANALYZE} ~ ' + feature, data=data).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    print(anova_table)\n",
    "\n",
    "\n",
    "    \n",
    "def analyze_numerical_feature(feature, data):\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(data[feature], data[VOLTAGE_TO_ANALYZE], alpha=0.5)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(VOLTAGE_TO_ANALYZE)\n",
    "    plt.title(f'Scatter Plot of {VOLTAGE_TO_ANALYZE} vs {feature}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # Correlation analysis\n",
    "    correlation, p_value = pearsonr(data[feature], data[VOLTAGE_TO_ANALYZE])\n",
    "    print(f\"Correlation between {feature} and {VOLTAGE_TO_ANALYZE}: {correlation}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    \n",
    "def convert_condition_id_to_beh_names(condition_id):\n",
    "    return CONDITION_BEH_MAPPING[condition_id]\n",
    "\n",
    "\n",
    "def filter_df_by_kwargs(df, **kwargs):\n",
    "    mask = pd.Series([True] * len(df))\n",
    "    for col, value in kwargs.items():\n",
    "        mask = mask & (df[col] == value)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def extract_number(value):\n",
    "    try:\n",
    "        import re\n",
    "        return float(re.findall(r'\\d+', value)[0])\n",
    "    except (IndexError, ValueError):\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "def retrieve_data(selected_channel, filt_ds_path, subjects_to_include, raw_ds_path, output_dir):\n",
    "    df = pd.DataFrame(columns=[\"max_voltage\", \"mean_voltage\", \"subject\", \"outcome\", \"task\", \"cue\", \"prev_outcome\"])\n",
    "    max_voltages = []\n",
    "    mean_voltages = []\n",
    "    subjects = []\n",
    "    outcomes = []\n",
    "    tasks = []\n",
    "    cues = []\n",
    "    prev_outcomes = []\n",
    "    subject_names = [d for d in os.listdir(filt_ds_path) if os.path.isdir(os.path.join(filt_ds_path, d)) and \"sub\" in d]\n",
    "    for subject_name in subject_names:\n",
    "        if subjects_to_include is not None:\n",
    "            if subject_name not in subjects_to_include:\n",
    "                continue\n",
    "        # load paths\n",
    "        good_epochs_path = os.path.join(filt_ds_path, \"derivatives\", \"mne-bids-pipeline\", subject_name, \"eeg\", f\"{subject_name}_task-casinos_proc-clean_epo.fif\")\n",
    "        raw_beh_path = os.path.join(raw_ds_path, subject_name, \"beh\", f\"{subject_name}_task-casinos_beh.tsv\")\n",
    "        raw_casinos_events = os.path.join(raw_ds_path, subject_name, \"eeg\", f\"{subject_name}_task-casinos_events.tsv\")\n",
    "        if any([not os.path.exists(good_epochs_path), not os.path.exists(raw_casinos_events),not os.path.exists(raw_beh_path)]):\n",
    "            print(f\"{subject_name} is missing either good_epochs_file, raw_beh_path or raw_casinos_events!\")\n",
    "            continue\n",
    "\n",
    "        # load files\n",
    "        epochs = mne.read_epochs(good_epochs_path, preload=True)\n",
    "        beh = pd.read_csv(raw_beh_path, sep='\\t')\n",
    "        beh['prev_outcome'] = beh['outcome'].shift(1)\n",
    "        raw_condition_event_ids = pd.read_csv(raw_casinos_events, sep='\\t')\n",
    "        raw_condition_event_ids[\"value\"] = raw_condition_event_ids[\"value\"].astype(str)\n",
    "\n",
    "        # pick channel\n",
    "        epochs = epochs.pick(picks=[selected_channel])\n",
    "        event_ids = epochs.events\n",
    "\n",
    "        for condition in epochs.event_id.keys():\n",
    "            condition_epochs = epochs[condition]\n",
    "            condition_event_ids = event_ids[event_ids[:, 2] == epochs.event_id[condition]]\n",
    "            condition_id = CONDITION_MAPPING[condition]\n",
    "            raw_condition_event_ids[\"number\"] = raw_condition_event_ids[\"value\"].apply(extract_number)\n",
    "            condition_condition_event_ids = raw_condition_event_ids[raw_condition_event_ids[\"number\"] == condition_id]\n",
    "            filtered_condition_idxs = [i for i, sample in enumerate(condition_condition_event_ids[\"sample\"]) if int(sample) - 1 in condition_event_ids[:, 0]]\n",
    "            task, prob, outcome = convert_condition_id_to_beh_names(condition_id)\n",
    "            filtered_beh = filter_df_by_kwargs(beh, task=task, prob=prob, outcome=outcome)\n",
    "            filtered_beh.index = range(len(filtered_beh))\n",
    "            filtered_beh = filtered_beh.loc[filtered_condition_idxs]\n",
    "            for i, epoch in enumerate(condition_epochs):\n",
    "                epoch = epoch.flatten() * 1000000\n",
    "                epoch = epoch[440:541]\n",
    "                max_voltage = np.max(epoch)\n",
    "                mean_voltage = np.mean(epoch)\n",
    "                outcome = \"win\" if filtered_beh.iloc[i][\"outcome\"] == 1 else \"loss\"\n",
    "                if filtered_beh.iloc[i][\"prev_outcome\"] == 1:\n",
    "                    prev_outcome = \"win\"\n",
    "                elif filtered_beh.iloc[i][\"prev_outcome\"] == 0:\n",
    "                    prev_outcome = \"loss\"\n",
    "                elif filtered_beh.iloc[i][\"prev_outcome\"] == -1:\n",
    "                    prev_outcome = \"invalid\"\n",
    "                else:\n",
    "                    raise Exception(\"Something went wrong with the prev_outcome assignment\")\n",
    "                if filtered_beh.iloc[i][\"task\"] == 1:\n",
    "                    task = \"low\"\n",
    "                elif filtered_beh.iloc[i][\"task\"] == 2:\n",
    "                    task = \"mid\"\n",
    "                elif filtered_beh.iloc[i][\"task\"] == 3:\n",
    "                    task = \"high\"\n",
    "                else:\n",
    "                    raise Exception(\"Something went wrong with the task assignment\")\n",
    "                cue = \"low\" if filtered_beh.iloc[i][\"prob\"] == 50 else \"high\"\n",
    "                mean_voltages.append(mean_voltage)\n",
    "                max_voltages.append(max_voltage)\n",
    "                cues.append(cue)\n",
    "                outcomes.append(outcome)\n",
    "                prev_outcomes.append(prev_outcome)\n",
    "                subjects.append(subject_name)\n",
    "                tasks.append(task)\n",
    "    df = pd.DataFrame({\n",
    "        \"subject\": subjects,\n",
    "        \"task\": tasks,\n",
    "        \"cue\": cues,\n",
    "        \"prev_outcome\": prev_outcomes,\n",
    "        \"outcome\": outcomes,\n",
    "        \"mean_voltage\": mean_voltages,\n",
    "        \"max_voltage\": max_voltages\n",
    "    })\n",
    "    if output_dir is not None:\n",
    "        df.to_csv(output_dir, index=False)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T17:33:59.294733Z",
     "start_time": "2024-03-29T17:33:59.256297Z"
    }
   },
   "id": "6a699184b989f22c",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start Linear Modeling\n",
    "\n",
    "Before we start creating a Linear Model, lets first see how our data, i.e. the mean/max voltages values, looks like. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7be867ffa03f5500"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /mnt/d/PycharmProjects/EEG2324Brownie/Dataset/ds004147-filtered/derivatives/mne-bids-pipeline/sub-27/eeg/sub-27_task-casinos_proc-clean_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -200.00 ...     600.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Adding metadata with 31 columns\n",
      "356 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mretrieve_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mselected_channel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSELECTED_CHANNEL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mOUTPUT_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_ds_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRAW_DS_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilt_ds_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFILT_DS_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubjects_to_include\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSUBJECTS_TO_INCLUDE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# pair up task and cue\u001B[39;00m\n\u001B[1;32m      4\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtask_cue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtask\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcue\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Cell \u001B[0;32mIn[30], line 153\u001B[0m, in \u001B[0;36mretrieve_data\u001B[0;34m(selected_channel, filt_ds_path, subjects_to_include, raw_ds_path, output_dir)\u001B[0m\n\u001B[1;32m    151\u001B[0m epoch \u001B[38;5;241m=\u001B[39m epoch\u001B[38;5;241m.\u001B[39mflatten() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000000\u001B[39m\n\u001B[1;32m    152\u001B[0m epoch \u001B[38;5;241m=\u001B[39m epoch[\u001B[38;5;241m440\u001B[39m:\u001B[38;5;241m541\u001B[39m]\n\u001B[0;32m--> 153\u001B[0m max_voltage \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m mean_voltage \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(epoch)\n\u001B[1;32m    155\u001B[0m outcome \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwin\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m filtered_beh\u001B[38;5;241m.\u001B[39miloc[i][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutcome\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/brownie/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2810\u001B[0m, in \u001B[0;36mmax\u001B[0;34m(a, axis, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2692\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_max_dispatcher)\n\u001B[1;32m   2693\u001B[0m \u001B[38;5;129m@set_module\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   2694\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue, initial\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue,\n\u001B[1;32m   2695\u001B[0m          where\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue):\n\u001B[1;32m   2696\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2697\u001B[0m \u001B[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001B[39;00m\n\u001B[1;32m   2698\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2808\u001B[0m \u001B[38;5;124;03m    5\u001B[39;00m\n\u001B[1;32m   2809\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2810\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaximum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2811\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/brownie/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "data = retrieve_data(selected_channel=SELECTED_CHANNEL, output_dir=OUTPUT_DIR, raw_ds_path=RAW_DS_PATH, filt_ds_path=FILT_DS_PATH, subjects_to_include=SUBJECTS_TO_INCLUDE)\n",
    "\n",
    "# pair up task and cue\n",
    "data['task_cue'] = data['task'] + '_' + data['cue']\n",
    "# pair up task_cue with outcome\n",
    "data['task_cue_prev_outcome'] = data['task_cue'] + '_' + data['prev_outcome']\n",
    "data[\"task_cue_prev_outcome_outcome\"] = data['task_cue_prev_outcome'] + \"_\" + data[\"outcome\"]\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.boxplot(data[VOLTAGE_TO_ANALYZE])\n",
    "plt.ylabel(f'{VOLTAGE_TO_ANALYZE}')\n",
    "plt.title(f'{VOLTAGE_TO_ANALYZE} Values Visualized as a Box Plot')\n",
    "plt.show()\n",
    "\n",
    "data['squared_errors'] = (data[VOLTAGE_TO_ANALYZE] - data[VOLTAGE_TO_ANALYZE].mean()) ** 2 \n",
    "sse = data['squared_errors'].sum()\n",
    "print(\"Sum of Squared Errors (SSE):\", sse)\n",
    "data[VOLTAGE_TO_ANALYZE].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T17:33:59.808068Z",
     "start_time": "2024-03-29T17:33:59.296910Z"
    }
   },
   "id": "10404b46c3b304ec",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "The calculated SSE is our baseline value regarding Linear Modeling. This is the value we get, when **no** independent variables are considered, i.e. the Linear Model only consists of the intercept which is represented as the mean. As a next step, we want to carefully select predictors for our Linear Model to reduce the SSE and thus understand the baseline variance we currently have better."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93f0e6eff7eceac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Potential Predictors\n",
    "\n",
    "Given the paper, 3 predictors can be derived that helps in understanding the variance of the RewP values, namely the Task-Cue Pair, the Outcome and the Performance. Even though we know from the paper, that these 3 predictors are good to be included into our linear modeling, as a sanity check, let us check the relationship to RewP anyway. The variables task and cue are paired together \n",
    "\n",
    "#### Task"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1d3403acd830226"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "analyze_categorical_feature(\"task_cue\", data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ca806fc96326db",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the box plots and the ANOVA table it is clear that the task variable has a significant effect on the RewP generated. A F-value of 20 indicates that there is large evidence against the null hypothesis (Every Task Group contributes the same to the RewP). As this F-value is also way under the standard significance level of 0.01 we thereby conclude, that the task is a good as a predictor for the linear modeling."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4b6f10b63e7755a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "analyze_categorical_feature(\"task_cue_prev_outcome_outcome\", data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec73bcc0d2c8852f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame containing the relevant columns\n",
    "contingency_table = pd.crosstab(data['prev_outcome'], data['outcome'])\n",
    "\n",
    "# Perform chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi-squared statistic:\", chi2)\n",
    "print(\"P-value:\", p)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(\"Expected frequencies table:\")\n",
    "print(expected)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2963e10a98d3bdba",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: write analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1d64b8737ddba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "analyze_categorical_feature(\"outcome\", data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a3d63147fea441e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ac580b55a8be253"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#analyze_numerical_feature(\"performance\", data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d74df407f902688",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: write analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8227187cabf8bfea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Self-chosen Predictors\n",
    "\n",
    "We have decided to look for two more potential predictors, namely the Response Time, i.e. how fast a participant pulls the arm of slot and the Previous Outcome. We hypothesize that both these predictors can be good candidates to understand the variance in the RewP. Let's start with the Response Time. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "261e67621daadb2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  1. Self-Chosen Predictor: Response Time\n",
    "\n",
    "We hypothesize that a participant clicks the button faster when confident in their choice. Conversely, when the participant did not learn a pattern yet for a cue or if there is no pattern to learn, i.e. for low cues, the participant might think about which choice to take leading to a longer response time. When participants are confident, i.e. they respond fast, this could influence the RewP generated.\n",
    "\n",
    "\n",
    "#### Sanity Check for Response Time\n",
    "\n",
    "In our sanity check we will **only consider the medium task**. In the medium task, where both types of cue are present, we hypothesise that, particularly in this type of casino, it should be rewarding for a participant to be presented with a cue for which the participant has already learned the pattern (represented by the assumed faster click), due to the presence of low cues, and therefore frustrating to learn an assumed pattern for a non-learnable cue. \n",
    "We use 3 metrics to check whether the data indicate faster response times on learned trials or not, namely Z-score, percentile of response time error, and absolute response time error."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d61e5bc8780275"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subjects_dir = 'Dataset/ds004147-filtered'\n",
    "subject_dirs = [d for d in os.listdir(subjects_dir) if os.path.isdir(os.path.join(subjects_dir, d)) and \"sub\" in d]\n",
    "\n",
    "dfs = []\n",
    "for subject_dir in subject_dirs:\n",
    "    df = create_df_from_beh_tsv(os.path.join(subjects_dir, subject_dir, \"beh\", f\"{subject_dir}_task-casinos_beh.tsv\"))\n",
    "    dfs.append(df)\n",
    "    \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Filter by medium task (task=2)\n",
    "df2 = df[df['task'] == 2]\n",
    "\n",
    "# Filter by cue type: Non-Learned (prob=50) and Learned (prob=80, optimal=outcome)\n",
    "df2_non_learned = df2[(df2['prob'] == 50) | ((df2['prob'] == 80) & (df2['outcome'] == 0))]\n",
    "df2_learned = df2[(df2['prob'] == 80) & (df2['optimal'] == 1) & (df2['outcome'] == 1)]\n",
    "\n",
    "# Combine data for box plot\n",
    "combined_data = pd.concat([df2_non_learned[\"rt_z_score\"], df2_learned[\"rt_z_score\"]], axis=1)\n",
    "combined_data.columns = ['Non-Learned', 'Learned']\n",
    "\n",
    "# Box plot for Z-score\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.boxplot([df2_non_learned[\"rt_z_score\"], df2_learned[\"rt_z_score\"]])\n",
    "plt.xticks([1, 2], ['Non-Learned', 'Learned'])\n",
    "plt.ylabel('Response Time Error (Z-score)')\n",
    "plt.title('Distribution of Response Time Errors (Z-score)')\n",
    "\n",
    "# Box plot for Percentile\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot([df2_non_learned[\"rt_percentile\"], df2_learned[\"rt_percentile\"]])\n",
    "plt.xticks([1, 2], ['Non-Learned', 'Learned'])\n",
    "plt.ylabel('Response Time Percentile')\n",
    "plt.title('Distribution of Response Time Errors (Percentile)')\n",
    "\n",
    "# Box plot for Residuals\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot([df2_non_learned[\"residuals\"], df2_learned[\"residuals\"]])\n",
    "plt.xticks([1, 2], ['Non-Learned', 'Learned'])\n",
    "plt.ylabel('Error in ms')\n",
    "plt.title('Distribution of Response Time Errors (absolute)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a033b946c802ce",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66891c2cba5f108e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Result: No significant differences in response time for learned and non-learned trials\n",
    "\n",
    "\n",
    "Based on the three box plots showing response time errors for each subject in the medium task, there do not appear to be significant differences between non-learned and learned trials. This suggests that our hypothesis that subjects will pull the lever faster when they are more confident is very likely incorrect.\n",
    "\n",
    "Several factors may contribute to this result. First, regardless of whether the subject already \"knows\" which arm to pull, there is an inherent processing time required to determine which action to take. This processing time is influenced by the number of different cues the subject has to remember.\n",
    "\n",
    "Second, our assumption that confidence leads to faster responses may also be inaccurate. It's worth noting that subjects receive no tangible benefit from faster responses. Therefore, the motivation to respond quickly may not be as important as we originally hypothesized.\n",
    "\n",
    "**Thus, we conclude that using response time as a predictor is most likely not a good choice for linear modeling, as no difference between learned and unlearned trials can be extracted.**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5289bf7531d8dc7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8a25cb8759c12f4e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b4a5ad4247788241"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Self-Chosen Predictor: Previous Outcome\n",
    "\n",
    "For this predictor we hypothesize that the previous outcome might affect the RewP generated on the next trial. For that we are going to create a categorical variable which can take the value 0 for a loss and 1 for a win on the previous outcome. Again, we are first going to plot a box plot to visualize how the previous outcome affects the RewP generated for each task and cue combination, i.e. low-low, mid-low, mid-high and high-high.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81321631e4fa55bc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "analyze_categorical_feature(\"prev_outcome\", data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aea039b14616dba4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: write analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c8544766f185dab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### After the analysis of all the potential predictors, we will select Task, Cue, Outcome, Performance and Previous Outcome as Predictors for the dependent Variable RewP!\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b45efc7c7659557c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remaining Predictors: Task, Cue, Outcome and the Ratio of correct learnable Trial\n",
    "\n",
    "For the remaining predictors, we don't have to conduct a sanity check, because from the paper it is clear that these predictors are do have a strong relationship with the RewP generated."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd19ac6dec410ead"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test For MultiCollinearity\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ba3b4126bec10a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "Numerical independent variables can be represented by just a single variable whereas for a categorical independent variable we have to create n - 1 dummy variables where n is the number of categories. This leads to the following variables:\n",
    "- Task: categorical; x1 and x2 to represent Low / Mid / High\n",
    "- Cue: categorical; x3 to represent Low / High\n",
    "- Outcome: categorical; x4 to represent Win / Loss\n",
    "- Performance: numerical; x5\n",
    "- Previous Outcome: categorical; x6 \n",
    "\n",
    "#### E(RewP) = β₀ + β₁x₁ + β₂x₂ + β₃x₃ + β₄x₄ + β₅x₅ + β₆x₆\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9d2e3f829ee5fa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = pd.get_dummies(data, columns=[\"task_cue_prev_outcome_outcome\"], drop_first=True) \n",
    "# model_formula = 'RewP ~ performance + task_cue_low_low + task_cue_mid_high + task_cue_mid_low + outcome_win + prev_outcome_win'\n",
    "model_formula = f'{VOLTAGE_TO_ANALYZE} ~ task_cue_low_low + task_cue_mid_high + task_cue_mid_low + outcome_win + prev_outcome_win'\n",
    "model = ols(formula=model_formula, data=test).fit()\n",
    "print(model.summary())\n",
    "\n",
    "residuals = model.resid\n",
    "SSE = np.sum(residuals ** 2)\n",
    "print(\"Sum of Squared Errors (SSE):\", SSE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6970a8e5a005760",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: write Interpretation of results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6673910d984b6360"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
